def forward(self, samples: NestedTensor, targets: List = None, **kw):
    """The forward expects a NestedTensor, which consists of:
        - samples.tensor: batched images, of shape [batch_size x 3 x H x W]
        - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels

    It returns a dict with the following elements:
        - "pred_logits": the classification logits (including no-object) for all queries.
                        Shape= [batch_size x num_queries x num_classes]
        - "pred_boxes": The normalized boxes coordinates for all queries, represented as
                        (center_x, center_y, width, height). These values are normalized in [0, 1],
                        relative to the size of each individual image (disregarding possible padding).
                        See PostProcess for information on how to retrieve the unnormalized bounding box.
        - "aux_outputs": Optional, only returned when auxilary losses are activated. It is a list of
                        dictionnaries containing the two above keys for each decoder layer.
    """
    if targets is None:
        captions = kw["captions"]
    else:
        captions = [t["caption"] for t in targets]

    # encoder texts
    tokenized = self.tokenizer(captions, padding="longest", return_tensors="pt").to(
        samples.device
    )
    (
        text_self_attention_masks,
        position_ids,
        cate_to_token_mask_list,
    ) = generate_masks_with_special_tokens_and_transfer_map(
        tokenized, self.specical_tokens, self.tokenizer
    )

    if text_self_attention_masks.shape[1] > self.max_text_len:
        text_self_attention_masks = text_self_attention_masks[
            :, : self.max_text_len, : self.max_text_len
        ]
        position_ids = position_ids[:, : self.max_text_len]
        tokenized["input_ids"] = tokenized["input_ids"][:, : self.max_text_len]
        tokenized["attention_mask"] = tokenized["attention_mask"][:, : self.max_text_len]
        tokenized["token_type_ids"] = tokenized["token_type_ids"][:, : self.max_text_len]

    # extract text embeddings
    if self.sub_sentence_present:
        tokenized_for_encoder = {k: v for k, v in tokenized.items() if k != "attention_mask"}
        tokenized_for_encoder["attention_mask"] = text_self_attention_masks
        tokenized_for_encoder["position_ids"] = position_ids
    else:
        # import ipdb; ipdb.set_trace()
        tokenized_for_encoder = tokenized

    bert_output = self.bert(**tokenized_for_encoder)  # bs, 195, 768

    encoded_text = self.feat_map(bert_output["last_hidden_state"])  # bs, 195, d_model
    text_token_mask = tokenized.attention_mask.bool()  # bs, 195
    # text_token_mask: True for nomask, False for mask
    # text_self_attention_masks: True for nomask, False for mask

    if encoded_text.shape[1] > self.max_text_len:
        encoded_text = encoded_text[:, : self.max_text_len, :]
        text_token_mask = text_token_mask[:, : self.max_text_len]
        position_ids = position_ids[:, : self.max_text_len]
        text_self_attention_masks = text_self_attention_masks[
            :, : self.max_text_len, : self.max_text_len
        ]

    text_dict = {
        "encoded_text": encoded_text,  # bs, 195, d_model
        "text_token_mask": text_token_mask,  # bs, 195
        "position_ids": position_ids,  # bs, 195
        "text_self_attention_masks": text_self_attention_masks,  # bs, 195,195
    }

    # import ipdb; ipdb.set_trace()
    if isinstance(samples, (list, torch.Tensor)):
        samples = nested_tensor_from_tensor_list(samples)
    if not hasattr(self, 'features') or not hasattr(self, 'poss'):
        self.set_image_tensor(samples)

    srcs = []
    masks = []
    for l, feat in enumerate(self.features):
        src, mask = feat.decompose()
        srcs.append(self.input_proj[l](src))
        masks.append(mask)
        assert mask is not None
    if self.num_feature_levels > len(srcs):
        _len_srcs = len(srcs)
        for l in range(_len_srcs, self.num_feature_levels):
            if l == _len_srcs:
                src = self.input_proj[l](self.features[-1].tensors)
            else:
                src = self.input_proj[l](srcs[-1])
            m = samples.mask
            mask = F.interpolate(m[None].float(), size=src.shape[-2:]).to(torch.bool)[0]
            pos_l = self.backbone[1](NestedTensor(src, mask)).to(src.dtype)
            srcs.append(src)
            masks.append(mask)
            self.poss.append(pos_l)

    input_query_bbox = input_query_label = attn_mask = dn_meta = None
    hs, reference, hs_enc, ref_enc, init_box_proposal = self.transformer(
        srcs, masks, input_query_bbox, self.poss, input_query_label, attn_mask, text_dict
    )

    # deformable-detr-like anchor update
    outputs_coord_list = []
    for dec_lid, (layer_ref_sig, layer_bbox_embed, layer_hs) in enumerate(
        zip(reference[:-1], self.bbox_embed, hs)
    ):
        layer_delta_unsig = layer_bbox_embed(layer_hs)
        layer_outputs_unsig = layer_delta_unsig + inverse_sigmoid(layer_ref_sig)
        layer_outputs_unsig = layer_outputs_unsig.sigmoid()
        outputs_coord_list.append(layer_outputs_unsig)
    outputs_coord_list = torch.stack(outputs_coord_list)

    # output
    outputs_class = torch.stack(
        [
            layer_cls_embed(layer_hs, text_dict)
            for layer_cls_embed, layer_hs in zip(self.class_embed, hs)
        ]
    )
    out = {"pred_logits": outputs_class[-1], "pred_boxes": outputs_coord_list[-1]}

    # # for intermediate outputs
    # if self.aux_loss:
    #     out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord_list)

    # # for encoder output
    # if hs_enc is not None:
    #     # prepare intermediate outputs
    #     interm_coord = ref_enc[-1]
    #     interm_class = self.transformer.enc_out_class_embed(hs_enc[-1], text_dict)
    #     out['interm_outputs'] = {'pred_logits': interm_class, 'pred_boxes': interm_coord}
    #     out['interm_outputs_for_matching_pre'] = {'pred_logits': interm_class, 'pred_boxes': init_box_proposal}
    unset_image_tensor = kw.get('unset_image_tensor', True)
    if unset_image_tensor:
        self.unset_image_tensor() ## If necessary
    return out